# ETL-and-Machine-Learning-Pipeline

## 1. Description

People make use of Twitter to ask for helping and reporting the situation when natural disastsers happen.
However, as not all Tweets are about natural disaster, it is important to clarify/distinguish whether or not it is about natural disaster.

This project consists of

   1) Constructing ETL Pipeline which eventually merges data on messages and categories, utilizing pandas and Sqlite.

  2) Contructing Machine Learning Pipeline which predicts multi-categories by implementing nltk and sklearn (K-Nearest Neighbors and/or Random Forest algorithms), taking the data from 1).

  3) Embeding 2) into a webapp, implementing Flask and Plotly.

## 2. Repository structure
This repository has three sub-directories:

1) app, where the main execute program (run.py) is.

2) data, where database generated by ETL pipeline and ETL program is.
(`ETL pipeline preparation.ipynb` just helps for understanding the ETL program structure so that it needs not to be executed.)

3) models, where machine learning model generated by ML pipeline and ML program is.
(`ML pipeline preparation.ipynb` just helps for understanding the ML program structure so that it needs not to be executed.)

## 3. Necessary packages

* sys
* pandas
* sqlalchemy
* joblib
* re
* nltk
* sklearn
* json
* ploty
* flask

## 4. Necessary steps for executing

Step 1) Generate database in the sub-directory 'data'. (The db name refering to YourDBname below is up to you.)

--> `python process_data.py disaster_messages.csv disaster_categories.csv YourDBname.db`

Step 2) Generate machine learning model in the sub-directory 'model'. (The model name refering to YourModel below is up to you.)

--> `python train_classifier.py ../data/YourDBname.db YourModel.pkl`

Step 3) Implement web app in the sub-directory 'app'.

--> `python run.py`

Step 4) Check the webpage in `http://0.0.0.0:3001`.

## 5. Details on the preprocessing input data

When tokenizing messages, the following steps are applied.

  1. Replacing url to string 'urlplaceholder' to treat url as the same word.
  2. Removing punctuation to consider only alphanumerics.
  3. Removing stopwords
  4. Lemmatizing words
  5. Deleting empty clearned messages.
  
With the tokenizer, Count vectorize and Tfidf are implemented to convert the messages into vectors in high dimension.

## 6. Discussion on ML Model

K-Nearest Neighbors (KNN) and Random Forest (RF) are tested, and RF performs a bit well.
You can find the performance comparison in `ML pipeline preparation.ipynb`.
Thus, only RF is in `train_classifier.py`. 



